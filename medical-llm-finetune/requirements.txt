# Medical LLM Fine-tuning Requirements
# For QLoRA fine-tuning on consumer GPUs (8GB+ VRAM)

# Core ML
torch>=2.1.0
transformers>=4.36.0
datasets>=2.16.0
accelerate>=0.25.0

# Parameter-Efficient Fine-Tuning
peft>=0.7.0
bitsandbytes>=0.41.0  # For 4-bit quantization

# Training utilities
trl>=0.7.0  # For SFTTrainer
einops>=0.7.0
scipy>=1.11.0

# Evaluation
scikit-learn>=1.3.0
evaluate>=0.4.0

# Utilities
tqdm>=4.66.0
pandas>=2.0.0
numpy>=1.24.0
sentencepiece>=0.1.99
protobuf>=4.25.0

# Optional: For faster training with Unsloth (if supported)
# unsloth @ git+https://github.com/unslothai/unsloth.git
